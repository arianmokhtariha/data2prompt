<!-- this markdown was generated by data2prompt -->
# ðŸ“Š Project Context: data2prompt
> This document provides a structured context of the project's codebase and data schema.
> It is optimized for LLMs to understand the project structure, file contents, and data formats efficiently.

> Generated on: 2026-02-27 18:05
> Settings: CSV Sample=50, SQL Sample=50, Line Limit=55, Seed=42

> Tokens: 12524 (est. via o200k_base)

## Project Structure
```text
ðŸ“‚ data2prompt/
    ðŸ“„ .gitignore
    ðŸ“„ LICENSE
    ðŸ“„ pyproject.toml
    ðŸ“„ README.md
    ðŸ“‚ .github/
        ðŸ“‚ workflows/
            ðŸ“„ tests.yml
    ðŸ“‚ .roo/
        ðŸ“‚ rules/
            ðŸ“„ project-instructions.md
        ðŸ“‚ skills/
            ðŸ“‚ defensive-file-processing/
                ðŸ“„ SKILL.md
            ðŸ“‚ llm-project-packager/
                ðŸ“„ SKILL.md
            ðŸ“‚ python-cli-standardizer/
                ðŸ“„ SKILL.md
            ðŸ“‚ sql-context-sampler/
                ðŸ“„ SKILL.md
    ðŸ“‚ src/
        ðŸ“‚ data2prompt/
            ðŸ“„ cli.py
            ðŸ“„ constants.py
            ðŸ“„ main.py
            ðŸ“„ parsers.py
            ðŸ“„ ui.py
            ðŸ“„ utils.py
            ðŸ“„ __init__.py
        ðŸ“‚ data2prompt.egg-info/
            ðŸ“„ dependency_links.txt
            ðŸ“„ entry_points.txt
            ðŸ“„ PKG-INFO
            ðŸ“„ requires.txt
            ðŸ“„ SOURCES.txt
            ðŸ“„ top_level.txt
    ðŸ“‚ tests/
        ðŸ“„ test_cli.py
        ðŸ“„ test_utils.py
        ðŸ“„ __init__.py
```
---

## FILE: .gitignore
```text
# Python cache
__pycache__/
*.py[cod]
*$py.class

# Environments
venv/
env/
.env

# Test data or generated outputs
*.csv

# Build files 
build/
dist/
*.egg-info/

# roo folder
.roo/
```

---

## FILE: LICENSE
```text
MIT License

Copyright (c) 2026 arianmokhtariha

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

---

## FILE: pyproject.toml
```toml
[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "data2prompt"
version = "0.1.0"
description = "A high-performance CLI tool to convert local data science workspaces into LLM-ready context."
readme = "README.md"
requires-python = ">=3.10"
license = {file = "LICENSE"}
authors = [
    {name = "Arian Mokhtariha", email = "arian1385mokhtarihaa@gmail.com"}
]
dependencies = [
    "pandas>=2.0.0",
    "openpyxl>=3.1.0",
    "tabulate>=0.9.0",
    "rich>=13.0.0",
    "tiktoken>=0.7.0",
]

[project.scripts]
data2prompt = "data2prompt.main:main"

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
]

```

---

## FILE: README.md
```markdown
# ðŸ“Š Data2Prompt

> **Turn your messy Data Science workspaces into clean, LLM-ready context.**

Data2Prompt is a blazing-fast CLI tool designed specifically for Data Analysts and Scientists. Unlike standard code-packagers, it intelligently parses Jupyter Notebooks, selectively samples heavy CSVs, and strips out token-wasting binary bloat, generating a perfectly structured Markdown file optimized for AI context windows (Claude, GPT-4o, Gemini).

## âœ¨ Features
* **ðŸ§  Smart Jupyter Parsing:** Extracts code, markdown, and text outputs while stripping out heavy Base64 images and raw HTML tables.
* **ðŸ”ª Multi-Format Sampling:** Intelligent sampling for CSV, SQL, and Excel files to preserve schema context while staying within token limits.
* **ðŸŒ³ Project Tree Mapping:** Generates a visual folder structure so the LLM understands your workspace architecture.
* **ðŸ›¡ï¸ Defensive Processing:** Automatically detects binary files via Null-byte checks and truncates oversized text files to prevent context-window poisoning.
* **ðŸ“Š Token-Aware Output:** Real-time token estimation using `tiktoken` (`o200k_base`) to ensure generated prompts fit your target LLM.
* **âš™ï¸ Professional CLI:** Robust argument handling with support for custom ignore patterns via `.data2promptignore`.

## ðŸš€ Quick Start

**Installation:**
```bash
git clone https://github.com/arianmokhtariha/data2prompt.git
cd data2prompt
pip install .
```

## ðŸ› ï¸ Developer Setup
Want to contribute or run the tests locally? Install the package with developer dependencies:

```bash
git clone https://github.com/arianmokhtariha/data2prompt.git
cd data2prompt
pip install -e .[dev]
```

Run the test suite:
```bash
pytest
```

## ðŸ—ï¸ Engineering Standards
This repository is built with production-grade software engineering principles:
* **Strict Type Hinting (PEP 484):** Fully typed function signatures across the CLI, Parsers, and UI layers.
* **Separation of Concerns:** Modular architecture separating the Orchestrator (`main.py`), Parsers (`parsers.py`), and TUI (`ui.py`).
* **Defensive Programming:** Memory-safe file reading with size-based truncation and Null-byte binary detection to prevent context-window poisoning.
* **Automated Testing:** Core logic and CLI argument merging are validated via `pytest`.
* **Continuous Integration:** GitHub Actions automatically verify codebase integrity on every push.
```

---

## FILE: .github\workflows\tests.yml
```yml
name: Python tests

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.10"
        cache: "pip"
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install .[dev]
        
    - name: Run Pytest
      run: |
        pytest

```

---

## FILE: .roo\rules\project-instructions.md
```markdown
# Data2Prompt: Project Vision & Engineering Standards

### Core Mission
The data2prompt repository is a high-performance Python utility designed to bridge the gap between complex local codebases and Large Language Model context windows. The core mission is to provide an intelligent, filtered, and token-aware representation of a project's structure and content. Because this codebase serves as a primary portfolio piece for technical recruiters, every contribution must reflect senior-level engineering maturity. This includes a strict adherence to the Separation of Concerns principle, where logic is distributed across a modular src-layout. The project prioritizes readability, maintainability, and the use of industry-standard patterns over monolithic scripting.

### Architectural Integrity
The architecture is deliberately modular, with distinct boundaries between command-line interface handling, specialized data parsing, and core orchestration. Maintenance of this structure is paramount; new logic should be integrated into existing specialized modules rather than bloating the main execution path. Technical decisions within the codebase favor defensive programming, particularly regarding resource management and user safety. This is evidenced by the established patterns for handling large-scale data, such as the memory-resident conversion and sampling of binary Excel files and the precise calculation of prompt density using modern tokenization standards (tiktoken/o200k_base).

### Professional Operation & UI
Operational excellence in this project is defined by a "Safe-by-Default" philosophy. The system is designed to respect project-specific exclusion rules and persistent ignore patterns (via .data2promptignore) to ensure that the generated output remains focused and clean. The user interface reflects this professional standard by utilizing high-fidelity terminal components (Rich library) to provide real-time feedback and structured data summaries. Developers working on this project must ensure that any new features align with these established behaviorsâ€”preserving the integrity of the CLI argument merging, the accuracy of token estimation, and the sophisticated visual feedback system that defines the data2prompt experience.
```

---

## FILE: .roo\skills\defensive-file-processing\SKILL.md
```markdown
---
name: defensive-file-processing
description: Safe file handling for arbitrary project files. Implements binary detection via Null byte checks and size-based truncation to prevent token poisoning and memory issues.
---

# defensive-file-processing

This skill provides a pattern for safely reading and processing files from arbitrary project directories, protecting against binary "noise" and massive text files.

## When to use
- When your tool reads files from a user-provided directory.
- When you want to prevent "Token Poisoning" (binary data in LLM prompts).
- When you need to handle massive text files without crashing or exceeding context limits.

## When NOT to use
- When you are only reading known, small configuration files.
- When you are processing files with a specific, trusted format.

## Workflow

### 1. Implement Binary Detection
Use a Null byte check on the first 1024 bytes of a file. This is a reliable way to identify binary files regardless of their extension.

```python
def is_binary(file_path):
    """Check if a file is binary by looking for a Null byte in the first 1024 bytes."""
    try:
        with open(file_path, 'rb') as f:
            chunk = f.read(1024)
            return b'\0' in chunk
    except:
        return True # Treat as binary if unreadable
```

### 2. Implement Size-Based Truncation
Set a hard limit on the size of files you will read. If a file exceeds this limit, truncate it and add a clear warning.

```python
MAX_FILE_SIZE_KB = 200

def process_file(file_path):
    file_size_kb = os.path.getsize(file_path) / 1024
    
    if file_size_kb > MAX_FILE_SIZE_KB:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            # Read only the first few lines or characters
            content = f.read(10000) 
            return f"{content}\n\n-- ... [File truncated due to size ({file_size_kb:.1f}KB)] ...\n"
    
    # Normal processing...
```

### 3. Use Robust Encoding
Always use `encoding='utf-8'` and `errors='ignore'` when reading text files to prevent crashes on non-standard characters.

```python
with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
    content = f.read()
```

## Best Practices
- **Extension Skipping**: Combine binary detection with a robust list of skipped extensions (e.g., `.exe`, `.zip`, `.pdf`).
- **Truncation Markers**: Always include the original file size in the truncation message so the user (and the LLM) understands the scale of the missing data.
- **Safety First**: If a file is unreadable or its type is ambiguous, default to skipping it or treating it as binary.

```

---

## FILE: .roo\skills\llm-project-packager\SKILL.md
```markdown
---
name: llm-project-packager
description: Package entire codebases into LLM-optimized Markdown. Includes directory tree generation, self-recursion prevention, and AI-specific metadata headers.
---

# llm-project-packager

This skill provides a pattern for creating "Context Snapshots" of entire projects, optimized for consumption by Large Language Models.

## When to use
- When you need to provide an LLM with a complete overview of a project's structure and content.
- When you want to automate the creation of project documentation for AI.
- When you need to ensure that generated Markdown files don't include themselves in future runs (self-recursion).

## When NOT to use
- For simple single-file projects.
- When you only need to share a specific code snippet.

## Workflow

### 1. Generate a Visual Tree
Create a text-based directory tree that respects ignore rules. This gives the LLM immediate context on the project's organization.

```python
def generate_tree(startpath, ignore_folders, ignore_files):
    tree = []
    for root, dirs, files in os.walk(startpath):
        # Filter ignored folders and files
        dirs[:] = [d for d in dirs if d not in ignore_folders]
        files = [f for f in files if f not in ignore_files]
        
        # Build tree structure (e.g., using indentation)
        level = root.replace(startpath, '').count(os.sep)
        indent = ' ' * 4 * level
        tree.append(f"{indent}{os.path.basename(root)}/")
        for f in files:
            tree.append(f"{indent}    {f}")
    return "\n".join(tree)
```

### 2. Prevent Self-Recursion
Add a unique "Generation Flag" to the top of your output files. Before processing any Markdown file, check for this flag to avoid infinite nesting.

```python
GENERATION_FLAG = "<!-- this markdown was generated by data2prompt -->"

def is_generated_file(file_path):
    if not file_path.endswith('.md'):
        return False
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            # Scan only the first 100 characters for efficiency
            return GENERATION_FLAG in f.read(100)
    except:
        return False
```

### 3. Add AI-Specific Metadata
Include a header that explains the document's purpose to the LLM, along with the settings used to generate it.

```markdown
<!-- this markdown was generated by data2prompt -->
# Project Context Snapshot
This document contains a packaged version of the project for LLM analysis.
Generated on: 2026-02-26
Settings: --csv-sample-size 50 --sql-sample-size 70
```

## Best Practices
- **Efficiency**: Only scan the first few characters of a file for the generation flag.
- **Clarity**: Use clear Markdown headers (`#`, `##`, `###`) to separate files and sections.
- **Metadata**: Always include the date and the CLI arguments used, as this helps the LLM understand the context of the data it's seeing.

```

---

## FILE: .roo\skills\python-cli-standardizer\SKILL.md
```markdown
---
name: python-cli-standardizer
description: Standardize Python CLI tools using argparse with robust default merging logic to prevent user inputs from overwriting essential core configurations.
---

# python-cli-standardizer

This skill provides a pattern for building professional Python CLI tools that maintain a set of "Core Defaults" (like essential ignore folders or file extensions) while still allowing user customization.

## When to use
- When building a CLI tool that requires default lists (e.g., `--ignore-folders`, `--skip-exts`).
- When you want to ensure that user-provided arguments *add to* rather than *replace* critical system defaults.
- When using `argparse` for configuration.

## When NOT to use
- For simple scripts where hardcoded values are sufficient.
- When user input should strictly override all defaults.

## Workflow

### 1. Define Core Constants
Define your essential defaults as sets to facilitate efficient merging and duplicate removal.

```python
CORE_IGNORES = {'.git', '__pycache__', '.venv', 'node_modules'}
CORE_SKIP_EXTS = {'.exe', '.bin', '.pyc'}
```

### 2. Setup Argparse with Empty Defaults
Set the `default` for list-based arguments to an empty list `[]`. This allows you to detect if the user provided any input.

```python
import argparse

def setup_cli():
    parser = argparse.ArgumentParser()
    parser.add_argument('--ignore-folders', nargs='+', default=[], help="Additional folders to ignore")
    return parser.parse_args()
```

### 3. Implement Merge Logic
In your main execution function, merge the user input with the core constants using set unions.

```python
def run_tool():
    args = setup_cli()
    
    # Merge logic: User Input | Core Defaults
    # This ensures essential ignores are never lost
    args.ignore_folders = list(set(args.ignore_folders) | CORE_IGNORES)
    
    # Proceed with merged configuration
    print(f"Effective ignores: {args.ignore_folders}")
```

## Best Practices
- **Use Sets**: Always use `set()` for merging to automatically handle duplicates.
- **Type Safety**: Ensure `CORE_IGNORE_FILES` is initialized as `set()` rather than `{}` (which creates a dictionary).
- **Transparency**: Log or print the final merged configuration so the user understands what is being ignored.

```

---

## FILE: .roo\skills\sql-context-sampler\SKILL.md
```markdown
---
name: sql-context-sampler
description: Intelligent SQL file processing for LLM context. Preserves database schemas (CREATE, ALTER) while sampling data rows (INSERT INTO) on a per-table basis to keep prompts concise yet informative.
---

# sql-context-sampler

This skill provides a specialized pattern for processing large SQL files to provide an LLM with the database schema and a representative sample of data without exceeding token limits.

## When to use
- When you need to include SQL database structures in an LLM prompt.
- When SQL files contain massive `INSERT INTO` statements that would bloat the context.
- When you want to ensure *every* table in a database is represented by at least a few rows of data.

## When NOT to use
- For small SQL files where full content is manageable.
- When the exact data values are critical for the LLM's task (e.g., debugging a specific row).

## Workflow

### 1. Detect Schema vs. Data
Use keyword matching to distinguish between schema definitions (which should be preserved) and data rows (which should be sampled).

```python
# Keywords for schema preservation
SCHEMA_KEYWORDS = ["CREATE ", "ALTER ", "TABLE ", "INDEX ", "VIEW ", "TRIGGER "]
```

### 2. Implement Per-Table Sampling
Reset a row counter whenever a new table definition is encountered. This ensures that even in a multi-table SQL file, every table gets its sample.

```python
def process_sql(file_path, sample_size):
    processed_lines = []
    table_row_count = 0
    is_truncated = False

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            # Reset counter on new table definition
            if any(kw in line.upper() for kw in ["CREATE TABLE ", "BEGIN TABLE "]):
                table_row_count = 0
                is_truncated = False
            
            # Detect data rows (INSERT INTO or values starting with '(')
            is_insert = line.strip().upper().startswith("INSERT INTO ")
            is_data_row = line.strip().startswith("(")

            if is_insert or is_data_row:
                if table_row_count < sample_size:
                    processed_lines.append(line)
                    table_row_count += 1
                elif not is_truncated:
                    processed_lines.append("-- ... [Table data truncated for brevity] ...\n")
                    is_truncated = True
                continue
            
            # Preserve schema lines
            processed_lines.append(line)
    
    return "".join(processed_lines)
```

## Best Practices
- **Keyword Collision**: Use trailing spaces in keywords (e.g., `"TABLE "`) to avoid matching table names that contain the word (e.g., `my_table`).
- **Multi-line Inserts**: Ensure your logic handles both single-line `INSERT` statements and multi-line value lists starting with `(`.
- **Truncation Markers**: Always add a clear comment like `-- ... [Truncated] ...` so the LLM knows data is missing.

```

---

## FILE: src\data2prompt\cli.py
```py
import argparse
from argparse import Namespace

from .constants import (
    CORE_IGNORES,
    CORE_IGNORE_FILES,
    CORE_SKIP_EXTS,
    DEFAULT_CSV_SAMPLE_SIZE,
    DEFAULT_SQL_SAMPLE_SIZE,
    DEFAULT_SQL_MAX_LINES,
    DEFAULT_MAX_LINES,
    DEFAULT_MAX_SHEETS,
    DEFAULT_SEED,
    DEFAULT_MAX_FILE_SIZE_KB,
    DEFAULT_OUTPUT_FILE
)

def setup_cli() -> Namespace:
    """Configures the Command Line Interface (CLI) for the tool.

    Defines all available flags and their help descriptions.

    Returns:
        argparse.Namespace: An object containing the merged user and core configurations.
    """
    parser = argparse.ArgumentParser(
        description="ðŸ“Š Data2Prompt: High-tech prompt packaging for Data Scientists."
    )
    
    # Output settings
    parser.add_argument('-o', '--output', default=DEFAULT_OUTPUT_FILE,
                        help=f'Name of the generated markdown file (default: {DEFAULT_OUTPUT_FILE})')
    
    # CSV sampling settings
    parser.add_argument('-s', '--csv-sample-size', type=int, default=DEFAULT_CSV_SAMPLE_SIZE,
                        help=f'Number of random rows to sample from CSVs (default: {DEFAULT_CSV_SAMPLE_SIZE})')
    parser.add_argument('--seed', type=int, default=DEFAULT_SEED,
                        help=f'Random seed for consistent CSV sampling (default: {DEFAULT_SEED})')
    
    # SQL sampling settings
    parser.add_argument('--sql-sample-size', type=int, default=DEFAULT_SQL_SAMPLE_SIZE,
                        help=f'Number of INSERT statements to keep in SQL files (default: {DEFAULT_SQL_SAMPLE_SIZE})')
    
    parser.add_argument('--sql-max-lines', type=int, default=DEFAULT_SQL_MAX_LINES,
                        help=f'Max non-data lines to keep in SQL files (default: {DEFAULT_SQL_MAX_LINES})')
    
    # Notebook settings
    parser.add_argument('--max-lines', type=int, default=DEFAULT_MAX_LINES,
                        help=f'Max lines of text output to keep per notebook cell (default: {DEFAULT_MAX_LINES})')
    
    # Excel settings
    parser.add_argument('--max-sheets', type=int, default=DEFAULT_MAX_SHEETS,
                        help=f'Max number of sheets to process in Excel files (default: {DEFAULT_MAX_SHEETS})')
    
    # Exclusions
    parser.add_argument('--ignore-folders', nargs='+', default=[],
                        help='Additional folders to skip entirely')
    
    parser.add_argument('--ignore-files', nargs='+', default=[],
                        help='Additional files to skip entirely')
    
    parser.add_argument('--max-file-size', type=int, default=DEFAULT_MAX_FILE_SIZE_KB,
                        help=f'Max file size in KB to read entirely (default: {DEFAULT_MAX_FILE_SIZE_KB}KB)')
    
    # file formats to ignore
    parser.add_argument('--skip-exts', nargs='+', default=[],
                        help='Additional file extensions to skip content for')
    
    args = parser.parse_args()
    
    # --- Argument Merging Logic ---
    # We combine the user's terminal input with our CORE constants.
    # This ensures that even if a user provides custom ignores, essential items
    # like '.git' or binary extensions are still respected.
    args.ignore_folders = list(set(args.ignore_folders) | CORE_IGNORES)
    args.ignore_files = list(set(args.ignore_files) | CORE_IGNORE_FILES)
    args.skip_exts = list(set(args.skip_exts) | CORE_SKIP_EXTS)
    
    return args

```

---

## FILE: src\data2prompt\constants.py
```py
# --- Core Defaults & Constants ---

# Folders matching these names are excluded from both the project tree and content processing.
CORE_IGNORES = {
    '.git', '__pycache__', 'venv', '.vscode', '.ipynb_checkpoints',
    'node_modules', '.idea', 'dist', 'build', '.mypy_cache',
    '.pytest_cache', 'target', '.docker', '.aws', '.gcloud'
}

# Specific filenames that should be excluded from the entire process.
CORE_IGNORE_FILES = set()

# Files with these extensions will have their names listed in the project tree,
# but their actual content will be skipped.
CORE_SKIP_EXTS = {
    # Data & Databases
    '.pbix', '.db', '.sqlite', '.sqlite3', '.parquet', '.pkl', '.pickle', '.feather', '.h5',
    # Compressed & Binary
    '.zip', '.tar', '.gz', '.7z', '.rar', '.exe', '.dll', '.so', '.bin',
    # Media
    '.png', '.jpg', '.jpeg', '.gif', '.svg', '.pdf', '.mp4', '.mp3', '.mov',
    # Environment & Secrets
    '.env', '.venv', '.pyc', '.ds_store'
}

# Default values for CLI arguments and processing functions
DEFAULT_CSV_SAMPLE_SIZE = 50
DEFAULT_SQL_SAMPLE_SIZE = 50  # Controls the number of INSERT/data rows kept per table in SQL files.
DEFAULT_SQL_MAX_LINES = 70    # Caps the total number of non-data lines (comments, setup, etc.) in SQL files.
DEFAULT_MAX_LINES = 55        # Max lines of text output to keep per notebook cell.
DEFAULT_MAX_SHEETS = 10       # Max number of sheets to process in Excel files.
DEFAULT_SEED = 42             # Random seed for consistent sampling.
DEFAULT_MAX_FILE_SIZE_KB = 200
DEFAULT_OUTPUT_FILE = 'PROMPT.md'

# A unique identifier added to the top of every generated Markdown file.
GENERATION_FLAG = "this markdown was generated by data2prompt"

```

---

## FILE: src\data2prompt\main.py
```py
import os
import sys
import pandas as pd
from pathlib import Path
from typing import Dict, Any
from .cli import setup_cli
from .parsers import process_csv, process_notebook, process_sql, process_excel
from .utils import is_binary, generate_tree, count_tokens, load_ignore_file
from .ui import ui
from .constants import GENERATION_FLAG

def get_ui_action(ext: str, skip_exts: list[str]) -> str:
    """Determines the UI action string based on file extension."""
    if ext in skip_exts: return "Skipping"
    elif ext == '.csv': return "Sampling CSV"
    elif ext == '.ipynb': return "Cleaning Notebook"
    elif ext == '.sql': return "Parsing SQL"
    elif ext in ['.xlsx', '.xls']: return "Extracting Excel"
    elif ext == '.md': return "Reading Markdown"
    return "Reading File"

def process_target_file(file_path: Path, args: Any) -> Dict[str, Any]:
    """Handles a single file and returns its content, tokens, and metadata."""
    ext = file_path.suffix.lower()
    result = {
        "content": "",
        "tokens": 0,
        "type": ext if ext else "text",
        "status": "Read",
        "stats_update": {},
        "skip_file": False
    }

    if ext in args.skip_exts:
        result["content"] = f"*Note: Binary/Heavy file ({ext}). Content skipped for brevity.*\n"
        result["status"] = "Skipped (Binary)"
        result["type"] = f"Binary ({ext})"
    elif ext == '.csv':
        content = process_csv(file_path, args.csv_sample_size, args.seed)
        result["content"] = content
        result["stats_update"]["csv_count"] = 1
        result["tokens"] = count_tokens(content)
        result["type"] = "CSV"
        result["status"] = "Sampled"
    elif ext == '.ipynb':
        content = process_notebook(file_path, args.max_lines)
        result["content"] = content
        result["stats_update"]["notebook_count"] = 1
        result["tokens"] = count_tokens(content)
        result["type"] = "Notebook"
        result["status"] = "Cleaned"
    elif ext == '.sql':
        content = process_sql(file_path, args.sql_sample_size, args.sql_max_lines)
        result["content"] = content
        result["stats_update"]["sql_count"] = 1
        result["tokens"] = count_tokens(content)
        result["type"] = "SQL"
        result["status"] = "Parsed"
    elif ext in ['.xlsx', '.xls']:
        excel_md, sheet_count = process_excel(file_path, args.csv_sample_size, args.max_sheets)
        result["content"] = excel_md
        result["stats_update"]["excel_count"] = 1
        result["stats_update"]["excel_sheets_count"] = sheet_count
        result["tokens"] = count_tokens(excel_md)
        result["type"] = f"Excel ({sheet_count} sheets)"
        result["status"] = "Extracted"
    elif ext == '.md':
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                if GENERATION_FLAG in f.read(100):
                    result["skip_file"] = True
                    return result
        except:
            pass

    # Fallback for .md and other text files
    if not result["content"] and not result["skip_file"]:
        if is_binary(file_path):
            result["content"] = f"*Note: Binary content detected in {ext if ext else 'unknown'} file. Content skipped.*"
            result["status"] = "Skipped (Binary)"
        else:
            file_size_kb = file_path.stat().st_size / 1024
            try:
                lang = ext[1:] if ext and ext != '.md' else 'markdown' if ext == '.md' else 'text'
                if file_size_kb > args.max_file_size:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        header_content = f.read(10 * 1024)
                        result["content"] = f"```{lang}\n{header_content}\n```"
                        result["content"] += f"\n-- [File truncated: Showing first 10KB because it exceeds the size limit ({args.max_file_size}KB) to save context] --\n"
                        result["tokens"] = count_tokens(result["content"])
                        result["status"] = "Truncated"
                else:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        file_text = f.read()
                        result["content"] = f"```{lang}\n{file_text}\n```"
                        result["tokens"] = count_tokens(result["content"])
            except Exception:
                result["content"] = "*Could not read file.*"
                result["status"] = "Error"

    return result

def run_packager():
    """
    The main entry point for the Data2Prompt CLI.
    Orchestrates the argument parsing, file discovery, content processing, and Markdown generation.
    """
    args = setup_cli() # Retrieve user settings from the terminal
    
    ui.print_header()
    project_path = Path.cwd()

    # Load project-specific ignores from .data2promptignore
    project_ignores = load_ignore_file(str(project_path))
    
    # Merge project-specific ignores into the existing ignore lists
    # We treat these as both folder and file ignores for maximum coverage
    args.ignore_folders = list(set(args.ignore_folders) | set(project_ignores))
    args.ignore_files = list(set(args.ignore_files) | set(project_ignores))
    
    # 1. Build the Header with Metadata
    md_content = [
        f"<!-- {GENERATION_FLAG} -->",
        f"# ðŸ“Š Project Context: {project_path.name}",
        f"> This document provides a structured context of the project's codebase and data schema.",
        f"> It is optimized for LLMs to understand the project structure, file contents, and data formats efficiently.\n",
        f"> Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}",
        f"> Settings: CSV Sample={args.csv_sample_size}, SQL Sample={args.sql_sample_size}, Line Limit={args.max_lines}, Seed={args.seed}\n"
    ]
    
    with ui.status("Generating project tree structure..."):
        md_content.append("## Project Structure")
        md_content.append("```text")
        tree_text = generate_tree(str(project_path), args.ignore_folders, args.ignore_files)
        md_content.append(tree_text)
        md_content.append("```\n---\n")
    
    ui.print_step(2, "ðŸ›  Analyzing and Extracting Project Data...")
    
    # Collect all files first to set progress bar total
    all_files = []
    for root, dirs, files in os.walk(project_path):
        dirs[:] = [d for d in dirs if d not in args.ignore_folders]
        for file in files:
            if file == args.output or file == Path(sys.argv[0]).name or file in args.ignore_files:
                continue
            all_files.append(Path(root) / file)

    stats = {
        "file_count": 0,
        "csv_count": 0,
        "notebook_count": 0,
        "sql_count": 0,
        "excel_count": 0,
        "excel_sheets_count": 0
    }
    
    # For the summary table
    processed_files_info = []

    with ui.progress_bar("[cyan]Processing files...", total=len(all_files)) as (progress, task):
        for file_path in all_files:
            relative_path = file_path.relative_to(project_path)
            ext = file_path.suffix.lower()
            stats["file_count"] += 1
            
            # Determine action for progress bar
            action = get_ui_action(ext, args.skip_exts)
            progress.update(task, description=f"[cyan]{action}: [bold]{relative_path}[/bold]")
            
            result = process_target_file(file_path, args)
            if result.get("skip_file"):
                progress.advance(task)
                continue

            md_content.append(f"## FILE: {relative_path}")
            md_content.append(result["content"])
            
            # Update stats
            for key, value in result["stats_update"].items():
                stats[key] += value
            
            processed_files_info.append({
                "name": str(relative_path),
                "type": result["type"],
                "tokens": result["tokens"],
                "status": result["status"]
            })
            
            md_content.append("\n---\n")
            progress.advance(task)

    ui.print_step(3, f"ðŸ’¾ Saving to {args.output}...")
    
    # Calculate tokens before final save
    full_content_temp = "\n".join(md_content)
    total_tokens = count_tokens(full_content_temp)
    
    # Insert token count into the header (after settings line)
    md_content.insert(6, f"> Tokens: {total_tokens} (est. via o200k_base)\n")
    
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write("\n".join(md_content))

    # Final File Size Check
    file_size_kb = Path(args.output).stat().st_size / 1024
    
    # Display Summary Table
    ui.print_summary_table(processed_files_info)

    # Final Success Panel
    ui.print_success_panel(args.output, file_size_kb, total_tokens, stats)
    
    if file_size_kb > 2000:
        ui.print_warning_panel(
            "âš ï¸  [bold yellow]WARNING:[/bold yellow] File is over 2MB. This might be too large for some context windows.\n"
            "ðŸ’¡ [bold cyan]Suggestion:[/bold cyan] Reduce --csv-sample-size, --sql-sample-size or --max-lines."
        )

if __name__ == "__main__":
    run_packager()

```

---

## FILE: src\data2prompt\parsers.py
```py
import json
from pathlib import Path
from typing import Tuple, Union

import openpyxl
import pandas as pd

from .constants import (
    DEFAULT_CSV_SAMPLE_SIZE,
    DEFAULT_SQL_SAMPLE_SIZE,
    DEFAULT_SQL_MAX_LINES,
    DEFAULT_MAX_LINES,
    DEFAULT_MAX_SHEETS,
    DEFAULT_SEED
)

def process_csv(
    file_path: Union[str, Path],
    sample_size: int = DEFAULT_CSV_SAMPLE_SIZE,
    seed: int = DEFAULT_SEED,
) -> str:
    try:
        df = pd.read_csv(file_path)
        if len(df) > sample_size:
            df = df.sample(sample_size, random_state=seed)
            footer = f"\n\n-- [CSV truncated: Showing random {sample_size} rows to save context] --"
        else:
            footer = ""
        return (
            f"#### [Sample - Random {sample_size} rows]\n"
            + df.to_markdown(index=False)
            + footer
        )
    except pd.errors.EmptyDataError:
        return "*Note: CSV file is empty.*"
    except Exception as e:
        return f"Error reading CSV: {e}"


def process_notebook(
    file_path: Union[str, Path], max_lines: int = DEFAULT_MAX_LINES
) -> str:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            nb = json.load(f)
        output_md = []
        
        # Added enumeration to track cell numbers
        for i, cell in enumerate(nb.get('cells', []), 1):
            cell_type = cell['cell_type'].upper()
            
            # Create a clear, readable header for the LLM
            output_md.append(f"### Cell {i} [{cell_type}]")
            
            if cell['cell_type'] == 'markdown':
                output_md.append("".join(cell['source']))
            
            elif cell['cell_type'] == 'code':
                code = "".join(cell['source'])
                output_md.append(f"```python\n{code}\n```")
                
                for out in cell.get('outputs', []):
                    if out.get('output_type') == 'stream':
                        text = "".join(out.get('text', []))
                        lines = text.strip().split('\n')
                        if len(lines) > max_lines:
                            truncated_text = '\n'.join(lines[:max_lines])
                            output_md.append(f"> **Cell {i} Output:**\n> {truncated_text}\n> -- [Output truncated: Showing first {max_lines} lines to save context] --")
                        else:
                            output_md.append(f"> **Cell {i} Output:**\n> {text.strip()}")
                    
                    elif out.get('output_type') in ['execute_result', 'display_data']:
                        data = out.get('data', {})
                        if 'text/plain' in data:
                            content = "".join(data['text/plain'])
                            if "base64" not in content:
                                lines = content.strip().split('\n')
                                if len(lines) > max_lines:
                                    truncated_content = '\n'.join(lines[:max_lines])
                                    output_md.append(f"> **Cell {i} Data Preview:**\n> {truncated_content}\n> -- [Data preview truncated: Showing first {max_lines} lines to save context] --")
                                else:
                                    output_md.append(f"> **Cell {i} Data Preview:**\n> {content.strip()}")
                                
            output_md.append("\n---\n") # Visual separator between cells
            
        return "\n\n".join(output_md)
    except json.JSONDecodeError:
        return "*Error: Malformed Jupyter Notebook (Invalid JSON).*"
    except Exception as e:
        return f"Error processing notebook: {e}"


def process_sql(
    file_path: Union[str, Path],
    sample_size: int = DEFAULT_SQL_SAMPLE_SIZE,
    max_lines: int = DEFAULT_SQL_MAX_LINES,
) -> str:
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        
        processed_lines = []
        table_row_count = 0
        is_truncated = False
        
        for line in lines:
            line_upper = line.upper()
            
            # 1. Detect New Table (Reset Counter)
            if "CREATE TABLE" in line_upper or "BEGIN TABLE" in line_upper:
                table_row_count = 0
                is_truncated = False
                processed_lines.append(line)
                continue

            # 2. Handle Inserts and Data Rows (Sample per table)
            is_insert = "INSERT INTO" in line_upper
            is_data_row = line.strip().startswith("(")
            
            if is_insert or is_data_row:
                if table_row_count < sample_size:
                    processed_lines.append(line)
                    table_row_count += 1
                elif not is_truncated:
                    processed_lines.append(f"-- [Table data truncated: Showing first {sample_size} rows to save context] --\n")
                    is_truncated = True
                continue
            
            # 3. Keep other schema keywords
            schema_keywords = ["ALTER ", "CONSTRAINT ", "VIEW ", "DROP ", "INDEX ", "TABLE "]
            if any(kw in line_upper for kw in schema_keywords):
                processed_lines.append(line)
                continue
                
            # 4. Keep other lines (comments, setup) up to the max_lines limit
            if len(processed_lines) < max_lines:
                processed_lines.append(line)
        
        return "```sql\n" + "".join(processed_lines) + "\n```"
    except Exception as e:
        return f"âš ï¸ Error reading SQL: {e}"

def process_excel(
    file_path: Union[str, Path],
    max_rows: int = DEFAULT_CSV_SAMPLE_SIZE,
    max_sheets: int = DEFAULT_MAX_SHEETS,
) -> Tuple[str, int]:
    try:
        # 1. Sheet Discovery & Visual Element Check using openpyxl
        wb = openpyxl.load_workbook(file_path, data_only=True, read_only=True)
        sheet_names = wb.sheetnames
        
        output_md = []
        processed_sheets = 0
        
        for sheet_name in sheet_names:
            if processed_sheets >= max_sheets:
                output_md.append(f"\n-- [Workbook truncated: Only first {max_sheets} sheets processed] --\n")
                break
            
            processed_sheets += 1
            sheet = wb[sheet_name]
            
            # Check for visual elements
            has_visuals = False
            try:
                if hasattr(sheet, '_images') and len(sheet._images) > 0:
                    has_visuals = True
                if hasattr(sheet, 'charts') and len(sheet.charts) > 0:
                    has_visuals = True
            except:
                pass

            # 2. Data Extraction using pandas
            try:
                df = pd.read_excel(file_path, sheet_name=sheet_name)
                
                output_md.append(f"### Sheet: {sheet_name}")
                
                if has_visuals:
                    output_md.append("*Note: Visual elements (images/charts) detected in this sheet.*")

                if df.empty:
                    output_md.append(f"*Note: Sheet '{sheet_name}' appears to be a visual dashboard or empty. No tabular data extracted.*")
                else:
                    # 3. Sampling (The Safety Guard)
                    if len(df) > max_rows:
                        df = df.head(max_rows)
                        footer = f"\n\n-- [Sheet truncated: Showing first {max_rows} rows to save context] --"
                    else:
                        footer = ""
                    
                    markdown_data = df.to_markdown(index=False)
                    output_md.append(markdown_data)
                    if footer:
                        output_md.append(footer)
            except Exception as e:
                output_md.append(f"### Sheet: {sheet_name}")
                output_md.append(f"âš ï¸ Error reading sheet data: {e}")
            
            output_md.append("\n---\n")
            
        wb.close()
        return "\n".join(output_md), processed_sheets
    except Exception as e:
        return f"âš ï¸ Error reading Excel: {e}", 0

```

---

## FILE: src\data2prompt\ui.py
```py
from contextlib import contextmanager
from typing import Any, Dict, Generator, List

from rich.console import Console, Group
from rich.panel import Panel
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
)
from rich.table import Table


class UIHandler:
    """
    Handles all Terminal User Interface (TUI) logic for Data2Prompt.
    Encapsulates Rich-based display components, formatting, and progress tracking.
    """
    def __init__(self) -> None:
        self.console = Console()

    def print_header(self) -> None:
        """Displays the application header."""
        header = "ðŸ“Š DATA PROJECT -> LLM PROMPT PACKAGER ðŸ“Š"
        self.console.print(Panel(header, style="bold blue", expand=False))

    def print_step(self, step_num: int, message: str) -> None:
        """Displays a formatted step message."""
        self.console.print(f"\n[bold blue]Step {step_num}: {message}[/bold blue]")

    @contextmanager
    def status(self, message: str) -> Generator[Any, None, None]:
        """Context manager for showing a status spinner."""
        with self.console.status(f"[bold green]{message}"):
            yield

    @contextmanager
    def progress_bar(
        self, description: str, total: int
    ) -> Generator[Any, None, None]:
        """Context manager for showing a progress bar."""
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=self.console,
        ) as progress:
            task = progress.add_task(description, total=total)
            yield progress, task

    def print_summary_table(self, processed_files_info: List[Dict[str, Any]]) -> None:
        """Displays a summary table of all processed files."""
        table = Table(title="Processing Summary", show_header=True, header_style="bold magenta")
        table.add_column("File Name", style="cyan")
        table.add_column("Type", style="green")
        table.add_column("Tokens", justify="right", style="yellow")
        table.add_column("Status", style="bold")

        for info in processed_files_info:
            status = info.get("status", "Unknown")
            status_color = "green" if status in ["Read", "Sampled", "Cleaned", "Parsed", "Extracted"] else \
                           "yellow" if status in ["Truncated", "Skipped (Binary)"] else "red"
            
            table.add_row(
                info.get("name", "Unknown"),
                info.get("type", "Unknown"),
                f"{info.get('tokens', 0):,}",
                f"[{status_color}]{status}[/{status_color}]"
            )
        self.console.print(table)

    def print_success_panel(
        self,
        output_path: str,
        file_size_kb: float,
        total_tokens: int,
        stats: Dict[str, int],
    ) -> None:
        """Displays the final success panel with project statistics."""
        stats_grid = Table.grid(padding=(0, 1))
        stats_grid.add_row("ðŸ“‚", f"Total Files: [bold]{stats.get('file_count', 0)}[/bold]")
        stats_grid.add_row("ðŸ“Š", f"CSVs Sampled: [bold]{stats.get('csv_count', 0)}[/bold]")
        stats_grid.add_row("ðŸ““", f"Notebooks Cleaned: [bold]{stats.get('notebook_count', 0)}[/bold]")
        stats_grid.add_row("ðŸ’¾", f"SQL Scripts Parsed: [bold]{stats.get('sql_count', 0)}[/bold]")
        stats_grid.add_row("ðŸ“ˆ", f"Excel Files Handled: [bold]{stats.get('excel_count', 0)}[/bold] ({stats.get('excel_sheets_count', 0)} sheets)")

        success_panel = Panel(
            Group(
                f"âœ… [bold green]DONE![/bold green] Created: [bold]{output_path}[/bold] ({file_size_kb:.1f} KB)",
                f"Tokens: [bold yellow]{total_tokens:,}[/bold yellow] (est. via o200k_base)",
                "",
                stats_grid
            ),
            border_style="green",
            title="Success"
        )
        self.console.print(success_panel)

    def print_warning_panel(self, message: str) -> None:
        """Displays a warning message in a panel."""
        self.console.print(Panel(message, border_style="yellow"))

    def print_warning(self, message: str) -> None:
        """Displays a simple warning message."""
        self.console.print(f"[yellow]âš ï¸  Warning: {message}[/yellow]")

    def print_error(self, message: str) -> None:
        """Displays an error message."""
        self.console.print(f"[red]âŒ Error: {message}[/red]")

# Global UI instance
ui = UIHandler()

```

---

## FILE: src\data2prompt\utils.py
```py
import os
import sys
from pathlib import Path
from typing import List, Union

import tiktoken

from .ui import ui


def count_tokens(text: str, encoding_name: str = "o200k_base") -> int:
    """Returns the number of tokens in a text string."""
    try:
        encoding = tiktoken.get_encoding(encoding_name)
        num_tokens = len(encoding.encode(text))
        return num_tokens
    except Exception:
        return 0

def is_binary(file_path: Union[str, Path]) -> bool:
    """Check if a file is binary by looking for a Null byte in the first 1024 bytes."""
    try:
        with open(file_path, "rb") as f:
            chunk = f.read(1024)
            return b"\0" in chunk
    except OSError:
        return False


def generate_tree(
    startpath: Union[str, Path], ignore_folders: List[str], ignore_files: List[str]
) -> str:
    tree = []
    for root, dirs, files in os.walk(startpath):
        dirs[:] = [d for d in dirs if d not in ignore_folders]
        level = root.replace(startpath, '').count(os.sep)
        indent = ' ' * 4 * level
        tree.append(f"{indent}ðŸ“‚ {os.path.basename(root)}/")
        sub_indent = ' ' * 4 * (level + 1)
        for f in files:
            if f not in ignore_files:
                tree.append(f"{sub_indent}ðŸ“„ {f}")
    return "\n".join(tree)

def load_ignore_file(directory: Union[str, Path]) -> List[str]:
    """
    Looks for a .data2promptignore file in the given directory.
    Returns a list of patterns to ignore, excluding comments and empty lines.
    """
    ignore_path = os.path.join(directory, '.data2promptignore')
    ignore_list = []
    
    if os.path.exists(ignore_path):
        try:
            with open(ignore_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    # Skip empty lines and comments
                    if not line or line.startswith('#'):
                        continue
                    # Strip trailing slashes and whitespace
                    pattern = line.rstrip('/')
                    ignore_list.append(pattern)
        except Exception as e:
            ui.print_warning(f"Could not read .data2promptignore: {e}")
            
    return ignore_list

```

---

## FILE: src\data2prompt\__init__.py
```py

```

---

## FILE: src\data2prompt.egg-info\dependency_links.txt
```txt


```

---

## FILE: src\data2prompt.egg-info\entry_points.txt
```txt
[console_scripts]
data2prompt = data2prompt.main:run_packager

```

---

## FILE: src\data2prompt.egg-info\PKG-INFO
```text
Metadata-Version: 2.4
Name: data2prompt
Version: 0.1.0
Summary: A high-performance Python utility to package local codebases and data for LLM context.
Author-email: Arian Mokhtariha <arian1385mokhtarihaa@gmail.com>
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: tiktoken>=0.9.0
Requires-Dist: pandas>=2.2.3
Requires-Dist: openpyxl>=3.1.5
Requires-Dist: rich>=13.9.4
Dynamic: license-file

# ðŸ“Š Data2Prompt

> **Turn your messy Data Science workspaces into clean, LLM-ready context.**

Data2Prompt is a blazing-fast CLI tool designed specifically for Data Analysts and Scientists. Unlike standard code-packagers, it intelligently parses Jupyter Notebooks, selectively samples heavy CSVs, and strips out token-wasting binary bloat, generating a perfectly structured Markdown file optimized for AI context windows (Claude, GPT-4o, Gemini).

## âœ¨ Features
* **ðŸ§  Smart Jupyter Parsing:** Extracts code, markdown, and text outputs while stripping out heavy Base64 images and raw HTML tables.
* **ðŸ”ª CSV Sampling:** Automatically samples large datasets to give LLMs a "taste" of your data schema without forgetting the main distribution.
* **ðŸŒ³ Project Tree Mapping:** Generates a visual folder structure so the LLM understands your workspace architecture.
* **ðŸ›¡ï¸ Binary Safe:** Automatically ignores `.pbix`, `.db`, `.zip`, and other heavy files.

## ðŸš€ Quick Start

**Installation:**
```bash
git clone [https://github.com/arianmokhtariha/data2prompt.git](https://github.com/arianmokhtariha/data2prompt.git)
cd data2prompt
pip install .

```

---

## FILE: src\data2prompt.egg-info\requires.txt
```txt
tiktoken>=0.9.0
pandas>=2.2.3
openpyxl>=3.1.5
rich>=13.9.4

```

---

## FILE: src\data2prompt.egg-info\SOURCES.txt
```txt
LICENSE
README.md
pyproject.toml
src/data2prompt/__init__.py
src/data2prompt/cli.py
src/data2prompt/constants.py
src/data2prompt/main.py
src/data2prompt/parsers.py
src/data2prompt/ui.py
src/data2prompt/utils.py
src/data2prompt.egg-info/PKG-INFO
src/data2prompt.egg-info/SOURCES.txt
src/data2prompt.egg-info/dependency_links.txt
src/data2prompt.egg-info/entry_points.txt
src/data2prompt.egg-info/requires.txt
src/data2prompt.egg-info/top_level.txt
```

---

## FILE: src\data2prompt.egg-info\top_level.txt
```txt
data2prompt

```

---

## FILE: tests\test_cli.py
```py
import sys
from unittest.mock import patch
from src.data2prompt.cli import setup_cli

def test_setup_cli_merges_defaults():
    # Simulate running the CLI with specific arguments
    test_args = ["data2prompt", "--ignore-folders", "custom_folder", "--skip-exts", ".foo"]
    
    with patch.object(sys, 'argv', test_args):
        args = setup_cli()
        
        # 1. User input should be present
        assert "custom_folder" in args.ignore_folders
        assert ".foo" in args.skip_exts
        
        # 2. CORE defaults must STILL be present (The Safe-by-Default feature)
        # These are defined in src/data2prompt/constants.py
        assert ".git" in args.ignore_folders
        assert ".exe" in args.skip_exts

```

---

## FILE: tests\test_utils.py
```py
import os
import tempfile
from pathlib import Path
from src.data2prompt.utils import count_tokens, is_binary, load_ignore_file

def test_count_tokens():
    # A simple string should return a deterministic token count
    text = "Hello, world! This is a test."
    tokens = count_tokens(text)
    assert isinstance(tokens, int)
    assert tokens > 0

def test_is_binary():
    with tempfile.NamedTemporaryFile(delete=False) as temp_bin:
        # Write a null byte to simulate a binary file
        temp_bin.write(b"Some text \x00 more text")
        temp_bin_path = temp_bin.name

    with tempfile.NamedTemporaryFile(mode='w', delete=False) as temp_text:
        # Write normal text
        temp_text.write("Just a normal text file.")
        temp_text_path = temp_text.name

    try:
        assert is_binary(temp_bin_path) is True
        assert is_binary(temp_text_path) is False
    finally:
        if os.path.exists(temp_bin_path):
            os.remove(temp_bin_path)
        if os.path.exists(temp_text_path):
            os.remove(temp_text_path)

def test_load_ignore_file():
    with tempfile.TemporaryDirectory() as temp_dir:
        ignore_path = Path(temp_dir) / ".data2promptignore"
        with open(ignore_path, "w") as f:
            f.write("# A comment\n")
            f.write("node_modules/\n")
            f.write("\n")
            f.write("secrets.json\n")
        
        ignores = load_ignore_file(temp_dir)
        assert "node_modules" in ignores  # Slash should be stripped
        assert "secrets.json" in ignores
        assert len(ignores) == 2

```

---

## FILE: tests\__init__.py
```py

```

---
