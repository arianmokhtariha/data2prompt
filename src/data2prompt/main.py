import os
import json
import pandas as pd
import sys
from pathlib import Path
import argparse


# --- Constants & Core Defaults ---
# These constants define the baseline behavior of the tool.
# User-provided arguments in the terminal will be merged with these defaults
# to ensure essential files and folders are always handled correctly.

# Folders matching these names are excluded from both the project tree and content processing.
# This includes version control, environment folders, and common IDE settings.
CORE_IGNORES = {
    '.git', '__pycache__', 'venv', '.vscode', '.ipynb_checkpoints',
    'node_modules', '.idea', 'dist', 'build', '.mypy_cache',
    '.pytest_cache', 'target', '.docker', '.aws', '.gcloud'
}

# Specific filenames that should be excluded from the entire process (tree and content).
# By default, this is empty to allow maximum flexibility, but can be populated with CORE defaults if needed.
CORE_IGNORE_FILES = set()

# Files with these extensions will have their names listed in the project tree,
# but their actual content will be skipped to avoid bloating the prompt with binary or heavy data.
CORE_SKIP_EXTS = {
    # Data & Databases
    '.pbix', '.db', '.sqlite', '.sqlite3', '.parquet', '.pkl', '.pickle', '.feather', '.h5',
    # Compressed & Binary
    '.zip', '.tar', '.gz', '.7z', '.rar', '.exe', '.dll', '.so', '.bin',
    # Media
    '.png', '.jpg', '.jpeg', '.gif', '.svg', '.pdf', '.mp4', '.mp3', '.mov',
    # Environment & Secrets
    '.env', '.venv', '.pyc', '.ds_store'
}

# A unique identifier added to the top of every generated Markdown file.
# This allows the tool to detect and skip its own previous outputs during subsequent runs.
GENERATION_FLAG = "this markdown was generated by data2prompt"

def setup_cli():
    """
    Configures the Command Line Interface (CLI) for the tool.
    Defines all available flags and their help descriptions.
    """
    parser = argparse.ArgumentParser(
        description="üìä Data2Prompt: High-tech prompt packaging for Data Scientists."
    )
    
    # Output settings
    parser.add_argument('-o', '--output', default='PROMPT.md',
                        help='Name of the generated markdown file (default: PROMPT.md)')
    
    # CSV sampling settings
    parser.add_argument('-s', '--csv-sample-size', type=int, default=70,
                        help='Number of random rows to sample from CSVs (default: 70)')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed for consistent CSV sampling (default: 42)')
    
    # SQL sampling settings
    parser.add_argument('--sql-sample-size', type=int, default=70,
                        help='Number of INSERT statements to keep in SQL files (default: 70)')
    
    # Notebook settings
    parser.add_argument('--max-lines', type=int, default=55,
                        help='Max lines of text output to keep per notebook cell (default: 55)')
    
    # Exclusions
    parser.add_argument('--ignore-folders', nargs='+', default=[],
                        help='Additional folders to skip entirely')
    
    parser.add_argument('--ignore-files', nargs='+', default=[],
                        help='Additional files to skip entirely')
    
    parser.add_argument('--max-file-size', type=int, default=200,
                        help='Max file size in KB to read entirely (default: 200KB)')
    
    # file formats to ignore
    parser.add_argument('--skip-exts', nargs='+', default=[],
                        help='Additional file extensions to skip content for')
    
    return parser.parse_args()


print("üöÄ Script is starting...")
def print_header():
    header = """
    ==============================================
        üìä DATA PROJECT -> LLM PROMPT PACKAGER üìä
    ==============================================
    """
    print(header)

def get_status_msg(file_path, file_count):
    sys.stdout.write(f"\r[#{file_count}] Processing: {file_path[:50]}...".ljust(70))
    sys.stdout.flush()

def is_binary(file_path):
    """Check if a file is binary by looking for a Null byte in the first 1024 bytes."""
    try:
        with open(file_path, 'rb') as f:
            chunk = f.read(1024)
            return b'\0' in chunk
    except Exception:
        return False

def generate_tree(startpath, ignore_folders, ignore_files):
    tree = []
    for root, dirs, files in os.walk(startpath):
        dirs[:] = [d for d in dirs if d not in ignore_folders]
        level = root.replace(startpath, '').count(os.sep)
        indent = ' ' * 4 * level
        tree.append(f"{indent}üìÇ {os.path.basename(root)}/")
        sub_indent = ' ' * 4 * (level + 1)
        for f in files:
            if f not in ignore_files:
                tree.append(f"{sub_indent}üìÑ {f}")
    return "\n".join(tree)

def process_csv(file_path, sample_size, seed=42):
    try:
        df = pd.read_csv(file_path)
        if len(df) > sample_size:
            df = df.sample(sample_size, random_state=seed)
        return f"#### [Sample - Random {sample_size} rows]\n" + df.to_markdown(index=False)
    except Exception as e:
        return f"Error reading CSV: {e}"
    
def process_notebook(file_path, max_lines):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            nb = json.load(f)
        output_md = []
        
        # Added enumeration to track cell numbers
        for i, cell in enumerate(nb.get('cells', []), 1):
            cell_type = cell['cell_type'].upper()
            
            # Create a clear, readable header for the LLM
            output_md.append(f"### Cell {i} [{cell_type}]")
            
            if cell['cell_type'] == 'markdown':
                output_md.append("".join(cell['source']))
            
            elif cell['cell_type'] == 'code':
                code = "".join(cell['source'])
                output_md.append(f"```python\n{code}\n```")
                
                for out in cell.get('outputs', []):
                    if out.get('output_type') == 'stream':
                        text = "".join(out.get('text', []))
                        if text.count('\n') < max_lines:
                            output_md.append(f"> **Cell {i} Output:**\n> {text.strip()}")
                    
                    elif out.get('output_type') in ['execute_result', 'display_data']:
                        data = out.get('data', {})
                        if 'text/plain' in data:
                            content = "".join(data['text/plain'])
                            if "base64" not in content and content.count('\n') < max_lines:
                                output_md.append(f"> **Cell {i} Data Preview:**\n> {content.strip()}")
                                
            output_md.append("\n---\n") # Visual separator between cells
            
        return "\n\n".join(output_md)
    except Exception as e:
        return f"Error processing notebook: {e}"

def process_sql(file_path, sample_size, max_lines):
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
        
        processed_lines = []
        table_row_count = 0
        is_truncated = False
        
        for line in lines:
            line_upper = line.upper()
            
            # 1. Detect New Table (Reset Counter)
            if "CREATE TABLE" in line_upper or "BEGIN TABLE" in line_upper:
                table_row_count = 0
                is_truncated = False
                processed_lines.append(line)
                continue

            # 2. Handle Inserts and Data Rows (Sample per table)
            is_insert = "INSERT INTO" in line_upper
            is_data_row = line.strip().startswith("(")
            
            if is_insert or is_data_row:
                if table_row_count < sample_size:
                    processed_lines.append(line)
                    table_row_count += 1
                elif not is_truncated:
                    processed_lines.append("-- ... [Table data truncated for brevity] ...\n")
                    is_truncated = True
                continue
            
            # 3. Keep other schema keywords
            schema_keywords = ["ALTER ", "CONSTRAINT ", "VIEW ", "DROP ", "INDEX ", "TABLE "]
            if any(kw in line_upper for kw in schema_keywords):
                processed_lines.append(line)
                continue
                
            # 4. Keep other lines (comments, setup) up to the max_lines limit
            if len(processed_lines) < max_lines:
                processed_lines.append(line)
        
        return "```sql\n" + "".join(processed_lines) + "\n```"
    except Exception as e:
        return f"‚ö†Ô∏è Error reading SQL: {e}"

def run_packager():
    """
    The main entry point for the Data2Prompt CLI.
    Orchestrates the argument parsing, file discovery, content processing, and Markdown generation.
    """
    args = setup_cli() # Retrieve user settings from the terminal
    
    # --- Argument Merging Logic ---
    # We combine the user's terminal input with our CORE constants.
    # This ensures that even if a user provides custom ignores, essential items
    # like '.git' or binary extensions are still respected.
    args.ignore_folders = list(set(args.ignore_folders) | CORE_IGNORES)
    args.ignore_files = list(set(args.ignore_files) | CORE_IGNORE_FILES)
    args.skip_exts = list(set(args.skip_exts) | CORE_SKIP_EXTS)
    
    print_header()
    project_path = os.getcwd()
    
    # 1. Build the Header with Metadata
    md_content = [
        f"<!-- {GENERATION_FLAG} -->",
        f"# üìä Project Context: {os.path.basename(project_path)}",
        f"> This document provides a structured context of the project's codebase and data schema.",
        f"> It is optimized for LLMs to understand the project structure, file contents, and data formats efficiently.\n",
        f"> Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}",
        f"> Settings: CSV Sample={args.csv_sample_size}, SQL Sample={args.sql_sample_size}, Line Limit={args.max_lines}, Seed={args.seed}\n"
    ]
    
    print("Step 1: üå≥ Generating project tree structure...")
    md_content.append("## Project Structure")
    md_content.append("```text")
    tree_text = generate_tree(project_path, args.ignore_folders, args.ignore_files)
    md_content.append(tree_text)
    md_content.append("```\n---\n")
    
    print("Step 2: üõ† Processing file contents...")
    file_count = 0
    csv_count = 0
    notebook_count = 0
    sql_count = 0

    for root, dirs, files in os.walk(project_path):
        dirs[:] = [d for d in dirs if d not in args.ignore_folders]
        for file in files:
            # Skip the output file itself, the script file, and ignored files
            if file == args.output or file == os.path.basename(sys.argv[0]) or file in args.ignore_files:
                continue
                
            file_path = Path(root) / file
            relative_path = file_path.relative_to(project_path)
            ext = file_path.suffix.lower()
            
            file_count += 1
            get_status_msg(str(relative_path), file_count)
            
            md_content.append(f"## FILE: {relative_path}")
            
            if ext in args.skip_exts:
                md_content.append(f"*Note: Binary/Heavy file ({ext}). Content skipped for brevity.*\n")
            elif ext == '.csv':
                md_content.append(process_csv(file_path, args.csv_sample_size, args.seed))
                csv_count += 1
            elif ext == '.ipynb':
                md_content.append(process_notebook(file_path, args.max_lines))
                notebook_count += 1
            elif ext == '.sql':
                md_content.append(process_sql(file_path, args.sql_sample_size, args.max_lines))
                sql_count += 1
            elif ext == '.md':
                # Check if the markdown file was generated by this tool to avoid self-recursion
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        if GENERATION_FLAG in f.read(100): # Check first 100 chars for efficiency
                            continue
                except:
                    pass
                
                # If not generated by us, treat as normal text file
                file_size_kb = os.path.getsize(file_path) / 1024
                try:
                    if file_size_kb > args.max_file_size:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            header_content = f.read(10 * 1024)
                            md_content.append(f"```markdown\n{header_content}\n```")
                            md_content.append(f"\n*Note: File truncated because it exceeds the size limit ({args.max_file_size}KB).*\n")
                    else:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            md_content.append(f"```markdown\n{f.read()}\n```")
                except Exception:
                    md_content.append("*Could not read file.*")
            else:
                # The New Safety Net: Check for binary content
                if is_binary(file_path):
                    md_content.append(f"*Note: Binary content detected in {ext if ext else 'unknown'} file. Content skipped.*")
                else:
                    file_size_kb = os.path.getsize(file_path) / 1024
                    try:
                        if file_size_kb > args.max_file_size:
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                # Read only the first 10KB to show the header
                                header_content = f.read(10 * 1024)
                                md_content.append(f"```{ext[1:] if ext else 'text'}\n{header_content}\n```")
                                md_content.append(f"\n*Note: File truncated because it exceeds the size limit ({args.max_file_size}KB).*\n")
                        else:
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                # Still use 'ignore' errors just in case of weird characters,
                                # but the binary check will stop the actual gibberish.
                                md_content.append(f"```{ext[1:] if ext else 'text'}\n{f.read()}\n```")
                    except Exception:
                        md_content.append("*Could not read file.*")
            
            md_content.append("\n---\n")

    print(f"\n\nStep 3: üíæ Saving to {args.output}...")
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write("\n".join(md_content))

    # Final File Size Check
    file_size_kb = os.path.getsize(args.output) / 1024
    
    print("\n" + "="*46)
    print(f"‚úÖ DONE! Created: {args.output} ({file_size_kb:.1f} KB)")
    print(f"üìÇ Total Files Processed: {file_count}")
    print(f"üìä CSVs Sampled:         {csv_count}")
    print(f"üìì Notebooks Cleaned:    {notebook_count}")
    print(f"üóÑÔ∏è SQL Scripts Parsed:   {sql_count}")
    
    if file_size_kb > 2000:
        print("‚ö†Ô∏è  WARNING: File is over 2MB. This might be too large for some context windows.")
        print("üí° Suggestion: Reduce --csv-sample-size, --sql-sample-size or --max-lines.")
    print("="*46)

if __name__ == "__main__":
    run_packager()