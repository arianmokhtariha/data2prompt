import os
import sys
import pandas as pd
from pathlib import Path

from .cli import setup_cli
from .parsers import process_csv, process_notebook, process_sql, process_excel
from .utils import is_binary, generate_tree, print_header, get_status_msg, count_tokens
from .constants import GENERATION_FLAG

def run_packager():
    """
    The main entry point for the Data2Prompt CLI.
    Orchestrates the argument parsing, file discovery, content processing, and Markdown generation.
    """
    args = setup_cli() # Retrieve user settings from the terminal
    
    print_header()
    project_path = os.getcwd()
    
    # 1. Build the Header with Metadata
    md_content = [
        f"<!-- {GENERATION_FLAG} -->",
        f"# üìä Project Context: {os.path.basename(project_path)}",
        f"> This document provides a structured context of the project's codebase and data schema.",
        f"> It is optimized for LLMs to understand the project structure, file contents, and data formats efficiently.\n",
        f"> Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}",
        f"> Settings: CSV Sample={args.csv_sample_size}, SQL Sample={args.sql_sample_size}, Line Limit={args.max_lines}, Seed={args.seed}\n"
    ]
    
    print("Step 1: üå≥ Generating project tree structure...")
    md_content.append("## Project Structure")
    md_content.append("```text")
    tree_text = generate_tree(project_path, args.ignore_folders, args.ignore_files)
    md_content.append(tree_text)
    md_content.append("```\n---\n")
    
    print("Step 2: üõ† Processing file contents...")
    file_count = 0
    csv_count = 0
    notebook_count = 0
    sql_count = 0
    excel_count = 0
    excel_sheets_count = 0

    for root, dirs, files in os.walk(project_path):
        dirs[:] = [d for d in dirs if d not in args.ignore_folders]
        for file in files:
            # Skip the output file itself, the script file, and ignored files
            if file == args.output or file == os.path.basename(sys.argv[0]) or file in args.ignore_files:
                continue
                
            file_path = Path(root) / file
            relative_path = file_path.relative_to(project_path)
            ext = file_path.suffix.lower()
            
            file_count += 1
            get_status_msg(str(relative_path), file_count)
            
            md_content.append(f"## FILE: {relative_path}")
            
            if ext in args.skip_exts:
                md_content.append(f"*Note: Binary/Heavy file ({ext}). Content skipped for brevity.*\n")
            elif ext == '.csv':
                md_content.append(process_csv(file_path, args.csv_sample_size, args.seed))
                csv_count += 1
            elif ext == '.ipynb':
                md_content.append(process_notebook(file_path, args.max_lines))
                notebook_count += 1
            elif ext == '.sql':
                md_content.append(process_sql(file_path, args.sql_sample_size, args.sql_max_lines))
                sql_count += 1
            elif ext in ['.xlsx', '.xls']:
                excel_md, sheet_count = process_excel(file_path, args.csv_sample_size, args.max_sheets)
                md_content.append(excel_md)
                excel_count += 1
                excel_sheets_count += sheet_count
                print(f"\r[Excel] Found {sheet_count} sheets in {file.ljust(40)}")
            elif ext == '.md':
                # Check if the markdown file was generated by this tool to avoid self-recursion
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        if GENERATION_FLAG in f.read(100): # Check first 100 chars for efficiency
                            continue
                except:
                    pass
                
                # If not generated by us, treat as normal text file
                file_size_kb = os.path.getsize(file_path) / 1024
                try:
                    if file_size_kb > args.max_file_size:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            header_content = f.read(10 * 1024)
                            md_content.append(f"```markdown\n{header_content}\n```")
                            md_content.append(f"\n*Note: File truncated because it exceeds the size limit ({args.max_file_size}KB).*\n")
                    else:
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            md_content.append(f"```markdown\n{f.read()}\n```")
                except Exception:
                    md_content.append("*Could not read file.*")
            else:
                # The New Safety Net: Check for binary content
                # Bypass for Excel files as we handle them
                if ext in ['.xlsx', '.xls']:
                    pass 
                elif is_binary(file_path):
                    md_content.append(f"*Note: Binary content detected in {ext if ext else 'unknown'} file. Content skipped.*")
                else:
                    file_size_kb = os.path.getsize(file_path) / 1024
                    try:
                        if file_size_kb > args.max_file_size:
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                # Read only the first 10KB to show the header
                                header_content = f.read(10 * 1024)
                                md_content.append(f"```{ext[1:] if ext else 'text'}\n{header_content}\n```")
                                md_content.append(f"\n*Note: File truncated because it exceeds the size limit ({args.max_file_size}KB).*\n")
                        else:
                            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                                # Still use 'ignore' errors just in case of weird characters,
                                # but the binary check will stop the actual gibberish.
                                md_content.append(f"```{ext[1:] if ext else 'text'}\n{f.read()}\n```")
                    except Exception:
                        md_content.append("*Could not read file.*")
            
            md_content.append("\n---\n")

    print(f"\n\nStep 3: üíæ Saving to {args.output}...")
    
    # Calculate tokens before final save
    full_content_temp = "\n".join(md_content)
    total_tokens = count_tokens(full_content_temp)
    
    # Insert token count into the header (after settings line)
    md_content.insert(6, f"> Tokens: {total_tokens} (est. via o200k_base)\n")
    
    with open(args.output, 'w', encoding='utf-8') as f:
        f.write("\n".join(md_content))

    # Final File Size Check
    file_size_kb = os.path.getsize(args.output) / 1024
    
    print("\n" + "="*46)
    print(f"‚úÖ DONE! Created: {args.output} ({file_size_kb:.1f} KB)")
    print(f"Tokens: {total_tokens} (est. via o200k_base)")
    print(f"üìÇ Total Files Processed: {file_count}")
    print(f"üìä CSVs Sampled:         {csv_count}")
    print(f"üìì Notebooks Cleaned:    {notebook_count}")
    print(f"üóÑÔ∏è SQL Scripts Parsed:   {sql_count}")
    print(f"üìà Excel Files Handled:  {excel_count}")
    print(f"üìë Excel Sheets Handled: {excel_sheets_count}")
    
    if file_size_kb > 2000:
        print("‚ö†Ô∏è  WARNING: File is over 2MB. This might be too large for some context windows.")
        print("üí° Suggestion: Reduce --csv-sample-size, --sql-sample-size or --max-lines.")
    print("="*46)

if __name__ == "__main__":
    run_packager()
